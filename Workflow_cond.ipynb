{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowcode\n",
    "import processing\n",
    "import res_flow_vis as visual\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filename associated with this specific run\n",
    "filename = \"testrun1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate a processor to handle data\n",
    "mpc = processing.Processor_cond(N_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load raw data\n",
    "Data, N_stars, M_stars, M_dm = mpc.get_data(\"all_sims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut out 0 of 95 galaxies, 4417968 of 34878379 stars (~13%).\n"
     ]
    }
   ],
   "source": [
    "#Clean data\n",
    "Data_const, N_stars_const, M_stars_const = mpc.constraindata(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose device\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters of the flow\n",
    "LAYER_TYPE = flowcode.NSF_CL2\n",
    "N_LAYERS = 8\n",
    "COND_INDS = np.array([10])\n",
    "DIM_COND = COND_INDS.shape[0]\n",
    "DIM_NOTCOND = Data_const[0].shape[1] - DIM_COND\n",
    "SPLIT = 0.5\n",
    "K = 8\n",
    "B = 3\n",
    "BASE_NETWORK = flowcode.MLP\n",
    "BASE_NETWORK_N_LAYERS = 4\n",
    "BASE_NETWORK_N_HIDDEN = 16\n",
    "BASE_NETWORK_LEAKY_RELU_SLOPE = 0.2\n",
    "\n",
    "SPLIT = {\"split\":SPLIT} if LAYER_TYPE == flowcode.NSF_CL else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "model = flowcode.NSFlow(N_LAYERS, DIM_NOTCOND, DIM_COND, LAYER_TYPE, **SPLIT, K=K, B=B, network=BASE_NETWORK, network_args=(BASE_NETWORK_N_HIDDEN,BASE_NETWORK_N_LAYERS,BASE_NETWORK_LEAKY_RELU_SLOPE))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training hyperparameters\n",
    "N_EPOCHS = 3\n",
    "INIT_LR = 0.01\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare data for flow\n",
    "Data_flow = mpc.Data_to_flow(mpc.diststack(Data_const))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save relevant data to the drive for external python file (device needs to be GPU) to do the training in background...\n",
    "torch.save(Data_flow, \"data_cond_trainer.pth\")\n",
    "torch.save(model, \"model_cond_trainer.pth\")\n",
    "np.save(\"params_cond_trainer.npy\", np.append(COND_INDS,np.array([N_EPOCHS,INIT_LR,BATCH_SIZE])))\n",
    "np.save(\"filename_cond_trainer.npy\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 89240, Loss:15.09046745300293, lr=0.01\n",
      "Step 100 of 89240, Loss:12.15351354598999, lr=0.009321643399992438\n",
      "Step 200 of 89240, Loss:8.996059532165527, lr=0.008689303567662258\n",
      "Step 300 of 89240, Loss:7.996769876480102, lr=0.008099858925202963\n",
      "Step 400 of 89240, Loss:8.193581714630128, lr=0.007550399649098802\n",
      "Step 500 of 89240, Loss:7.52669587135315, lr=0.007038213305632705\n",
      "Step 600 of 89240, Loss:7.526951999664306, lr=0.006560771460819006\n",
      "Step 700 of 89240, Loss:7.291182794570923, lr=0.0061157171986602225\n",
      "Step 800 of 89240, Loss:7.254281368255615, lr=0.005700853486111131\n",
      "Step 900 of 89240, Loss:6.803214254379273, lr=0.0053141323273131705\n",
      "Step 1000 of 89240, Loss:7.283186197280884, lr=0.004953644653558525\n",
      "Step 1100 of 89240, Loss:6.582707529067993, lr=0.004617610899075165\n",
      "Step 1200 of 89240, Loss:6.653057689666748, lr=0.004304372216109717\n",
      "Step 1300 of 89240, Loss:6.575308990478516, lr=0.004012382285940997\n",
      "Step 1400 of 89240, Loss:6.3289796733856205, lr=0.003740199685398845\n",
      "Step 1500 of 89240, Loss:6.595316743850708, lr=0.003486480771205193\n",
      "Step 1600 of 89240, Loss:6.188295793533325, lr=0.003249973047010543\n",
      "Step 1700 of 89240, Loss:6.092796974182129, lr=0.003029508980381914\n",
      "Step 1800 of 89240, Loss:6.036378965377808, lr=0.0028240002392194885\n",
      "Step 1900 of 89240, Loss:6.308166742324829, lr=0.002632432319149741\n",
      "Step 2000 of 89240, Loss:5.925738973617554, lr=0.0024538595353728967\n",
      "Step 2100 of 89240, Loss:5.75776312828064, lr=0.0022874003542417273\n",
      "Step 2200 of 89240, Loss:5.753211917877198, lr=0.0021322330415257757\n",
      "Step 2300 of 89240, Loss:5.6523850059509275, lr=0.0019875916058784537\n",
      "Step 2400 of 89240, Loss:5.5819358634948735, lr=0.0018527620174817253\n",
      "Step 2500 of 89240, Loss:5.438306083679199, lr=0.0017270786832015198\n",
      "Step 2600 of 89240, Loss:5.6312044239044186, lr=0.0016099211608533077\n",
      "Step 2700 of 89240, Loss:5.343634586334229, lr=0.00150071109635764\n",
      "Step 2800 of 89240, Loss:5.329327640533447, lr=0.001398909368665761\n",
      "Step 2900 of 89240, Loss:5.249136619567871, lr=0.0013040134283610776\n",
      "Step 3000 of 89240, Loss:5.206192388534546, lr=0.0012155548167983548\n",
      "Step 3100 of 89240, Loss:5.051297988891601, lr=0.0011330968535337399\n",
      "Step 3200 of 89240, Loss:5.067039051055908, lr=0.0010562324806294985\n",
      "Step 3300 of 89240, Loss:5.063971719741821, lr=0.0009845822531917602\n",
      "Step 3400 of 89240, Loss:5.005388422012329, lr=0.0009177924662214655\n",
      "Step 3500 of 89240, Loss:4.870327444076538, lr=0.0008555334085316102\n",
      "Step 3600 of 89240, Loss:4.834413652420044, lr=0.0007974977351111717\n",
      "Step 3700 of 89240, Loss:4.766000366210937, lr=0.0007433989499007971\n",
      "Step 3800 of 89240, Loss:4.775217695236206, lr=0.0006929699914904072\n",
      "Step 3900 of 89240, Loss:4.6698846340179445, lr=0.000645961914756937\n",
      "Step 4000 of 89240, Loss:4.659987878799439, lr=0.0006021426619340478\n",
      "Step 4100 of 89240, Loss:4.56887490272522, lr=0.0005612959170471394\n",
      "Step 4200 of 89240, Loss:4.58987229347229, lr=0.000523220038058517\n",
      "Step 4300 of 89240, Loss:4.56444766998291, lr=0.0004877270614511966\n",
      "Step 4400 of 89240, Loss:4.471438798904419, lr=0.0004546417743374252\n",
      "Step 4500 of 89240, Loss:4.4375386428833, lr=0.0004238008495113311\n",
      "Step 4600 of 89240, Loss:4.406321325302124, lr=0.00039505203917584867\n",
      "Step 4700 of 89240, Loss:4.434843187332153, lr=0.0003682534233637103\n",
      "Step 4800 of 89240, Loss:4.41980052947998, lr=0.00034327270934229507\n",
      "Step 4900 of 89240, Loss:4.41719841003418, lr=0.0003199865785438127\n",
      "Step 5000 of 89240, Loss:4.431594944000244, lr=0.00029828007779690927\n",
      "Step 5100 of 89240, Loss:4.738668575286865, lr=0.00027804605185447904\n",
      "Step 5200 of 89240, Loss:4.371376323699951, lr=0.00025918461441632595\n",
      "Step 5300 of 89240, Loss:4.360188207626343, lr=0.00024160265503535297\n",
      "Step 5400 of 89240, Loss:4.322144136428833, lr=0.0002252133794730947\n",
      "Step 5500 of 89240, Loss:4.303383355140686, lr=0.00020993588123553652\n",
      "Step 5600 of 89240, Loss:4.25753339767456, lr=0.00019569474217408353\n",
      "Step 5700 of 89240, Loss:4.237562789916992, lr=0.00018241966018002674\n",
      "Step 5800 of 89240, Loss:4.218736548423767, lr=0.00017004510213460092\n",
      "Step 5900 of 89240, Loss:4.20060881614685, lr=0.00015850998040140425\n",
      "Step 6000 of 89240, Loss:4.1831267786026, lr=0.00014775735126416805\n",
      "Step 6100 of 89240, Loss:4.174770603179931, lr=0.00013773413382119965\n",
      "Step 6200 of 89240, Loss:4.188784513473511, lr=0.0001283908479488061\n",
      "Step 6300 of 89240, Loss:4.189498343467712, lr=0.00011968137004014206\n",
      "Step 6400 of 89240, Loss:4.1777650833129885, lr=0.00011156270531367429\n",
      "Step 6500 of 89240, Loss:4.158280844688416, lr=0.00010399477556725128\n",
      "Step 6600 of 89240, Loss:4.143288068771362, lr=9.694022133001626e-05\n",
      "Step 6700 of 89240, Loss:4.158584413528442, lr=9.036421743547521e-05\n",
      "Step 6800 of 89240, Loss:4.158182625770569, lr=8.423430110528791e-05\n",
      "Step 6900 of 89240, Loss:4.09999596118927, lr=7.852021169510828e-05\n",
      "Step 7000 of 89240, Loss:4.1426382684707646, lr=7.319374131137152e-05\n",
      "Step 7100 of 89240, Loss:4.130885858535766, lr=6.822859556159e-05\n",
      "Step 7200 of 89240, Loss:4.136375107765198, lr=6.360026375074486e-05\n",
      "Step 7300 of 89240, Loss:4.102648148536682, lr=5.928589788299092e-05\n",
      "Step 7400 of 89240, Loss:4.114398231506348, lr=5.5264199871360786e-05\n",
      "Step 7500 of 89240, Loss:4.110240793228149, lr=5.15153163986733e-05\n",
      "Step 7600 of 89240, Loss:4.077211112976074, lr=4.80207409106215e-05\n",
      "Step 7700 of 89240, Loss:4.075154132843018, lr=4.4763222257224166e-05\n",
      "Step 7800 of 89240, Loss:4.092841401100158, lr=4.172667953164482e-05\n",
      "Step 7900 of 89240, Loss:4.07927472114563, lr=3.8896122685975645e-05\n",
      "Step 8000 of 89240, Loss:4.075302762985229, lr=3.6257578532102096e-05\n",
      "Step 8100 of 89240, Loss:4.072295017242432, lr=3.37980217623477e-05\n",
      "Step 8200 of 89240, Loss:4.099543018341064, lr=3.150531064937891e-05\n",
      "Step 8300 of 89240, Loss:4.079794154167176, lr=2.9368127107949438e-05\n",
      "Step 8400 of 89240, Loss:4.055746130943298, lr=2.7375920822595584e-05\n",
      "Step 8500 of 89240, Loss:4.047767443656921, lr=2.551885716546636e-05\n",
      "Step 8600 of 89240, Loss:4.083581247329712, lr=2.378776864718192e-05\n",
      "Step 8700 of 89240, Loss:4.072657389640808, lr=2.217410966105503e-05\n",
      "Step 8800 of 89240, Loss:4.052028179168701, lr=2.066991429726821e-05\n",
      "Step 8900 of 89240, Loss:4.051062092781067, lr=1.9267757018753954e-05\n",
      "Step 9000 of 89240, Loss:4.048524312973022, lr=1.7960716004652575e-05\n",
      "Step 9100 of 89240, Loss:4.037437176704406, lr=1.6742338980390822e-05\n",
      "Step 9200 of 89240, Loss:4.0489156579971315, lr=1.560661136569962e-05\n",
      "Step 9300 of 89240, Loss:4.0196634912490845, lr=1.454792658333208e-05\n",
      "Step 9400 of 89240, Loss:4.0812816286087035, lr=1.35610583819092e-05\n",
      "Step 9500 of 89240, Loss:4.032625985145569, lr=1.2641135036263599e-05\n",
      "Step 9600 of 89240, Loss:4.043211431503296, lr=1.1783615297919973e-05\n",
      "Step 9700 of 89240, Loss:4.03186722278595, lr=1.0984265976990563e-05\n",
      "Step 9800 of 89240, Loss:4.068562092781067, lr=1.0239141044817559e-05\n",
      "Step 9900 of 89240, Loss:4.053869466781617, lr=9.544562154201524e-06\n",
      "Step 10000 of 89240, Loss:4.0411088514328, lr=8.897100481053021e-06\n",
      "Step 10100 of 89240, Loss:4.046187009811401, lr=8.293559797827744e-06\n",
      "Step 10200 of 89240, Loss:4.025221462249756, lr=7.730960695186357e-06\n",
      "Step 10300 of 89240, Loss:4.025110201835632, lr=7.206525873988485e-06\n",
      "Step 10400 of 89240, Loss:4.023917565345764, lr=6.717666435013949e-06\n",
      "Step 10500 of 89240, Loss:4.008522305488587, lr=6.26196909872985e-06\n",
      "Step 10600 of 89240, Loss:4.028985624313354, lr=5.83718429201317e-06\n",
      "Step 10700 of 89240, Loss:4.044108271598816, lr=5.4412150430184096e-06\n",
      "Step 10800 of 89240, Loss:4.029615540504455, lr=5.072106629369212e-06\n",
      "Step 10900 of 89240, Loss:4.008565635681152, lr=4.728036928571739e-06\n",
      "Step 11000 of 89240, Loss:4.046136627197265, lr=4.407307423014126e-06\n",
      "Step 11100 of 89240, Loss:4.014681444168091, lr=4.108334815147729e-06\n",
      "Step 11200 of 89240, Loss:4.0261999702453615, lr=3.829643211458097e-06\n",
      "Step 11300 of 89240, Loss:4.006376752853393, lr=3.5698568366414212e-06\n",
      "Step 11400 of 89240, Loss:4.023420000076294, lr=3.3276932420196382e-06\n",
      "Step 11500 of 89240, Loss:4.016607894897461, lr=3.1019569746671787e-06\n",
      "Step 11600 of 89240, Loss:4.013683075904846, lr=2.89153367599668e-06\n",
      "Step 11700 of 89240, Loss:4.057006096839904, lr=2.6953845806710314e-06\n",
      "Step 11800 of 89240, Loss:4.016289262771607, lr=2.51254138868535e-06\n",
      "Step 11900 of 89240, Loss:4.010625872612, lr=2.342101485304662e-06\n",
      "Step 12000 of 89240, Loss:4.02054181098938, lr=2.183223485260268e-06\n",
      "Step 12100 of 89240, Loss:4.001668176651001, lr=2.0351230792084863e-06\n",
      "Step 12200 of 89240, Loss:4.039652013778687, lr=1.8970691619476067e-06\n",
      "Step 12300 of 89240, Loss:4.0344117069244385, lr=1.7683802232798093e-06\n",
      "Step 12400 of 89240, Loss:4.043555541038513, lr=1.6484209837013386e-06\n",
      "Step 12500 of 89240, Loss:3.995630407333374, lr=1.536599258312862e-06\n",
      "Step 12600 of 89240, Loss:4.007823314666748, lr=1.4323630334685366e-06\n",
      "Step 12700 of 89240, Loss:4.0265865230560305, lr=1.3351977417325131e-06\n",
      "Step 12800 of 89240, Loss:4.015932445526123, lr=1.2446237216905687e-06\n",
      "Step 12900 of 89240, Loss:3.9948130226135254, lr=1.1601938500770915e-06\n",
      "Step 13000 of 89240, Loss:4.0135297679901125, lr=1.0814913345282934e-06\n",
      "Step 13100 of 89240, Loss:4.0161492013931275, lr=1.008127656065468e-06\n",
      "Step 13200 of 89240, Loss:4.024160704612732, lr=9.397406511512511e-07\n",
      "Step 13300 of 89240, Loss:4.000542664527893, lr=8.759927238508655e-07\n",
      "Step 13400 of 89240, Loss:3.9951049947738646, lr=8.165691792725817e-07\n",
      "Step 13500 of 89240, Loss:4.039299449920654, lr=7.611766700603503e-07\n",
      "Step 13600 of 89240, Loss:4.032872657775879, lr=7.095417482696285e-07\n",
      "Step 13700 of 89240, Loss:4.023788485527039, lr=6.614095154776676e-07\n",
      "Step 13800 of 89240, Loss:4.035024738311767, lr=6.165423644644597e-07\n",
      "Step 13900 of 89240, Loss:4.001633419990539, lr=5.747188062525862e-07\n",
      "Step 14000 of 89240, Loss:3.9917386054992674, lr=5.357323767155953e-07\n",
      "Step 14100 of 89240, Loss:4.028662109375, lr=4.99390617357319e-07\n",
      "Step 14200 of 89240, Loss:4.00716338634491, lr=4.655141252307001e-07\n",
      "Step 14300 of 89240, Loss:4.015587339401245, lr=4.3393566730600085e-07\n",
      "Step 14400 of 89240, Loss:4.016992230415344, lr=4.044993549164297e-07\n",
      "Step 14500 of 89240, Loss:4.049374685287476, lr=3.770598742057935e-07\n",
      "Step 14600 of 89240, Loss:4.012663135528564, lr=3.514817687792413e-07\n",
      "Step 14700 of 89240, Loss:4.01898196220398, lr=3.2763877101586824e-07\n",
      "Step 14800 of 89240, Loss:3.9983054113388063, lr=3.054131787421702e-07\n",
      "Step 14900 of 89240, Loss:4.008119797706604, lr=2.8469527418926613e-07\n",
      "Step 15000 of 89240, Loss:4.028400378227234, lr=2.6538278236554087e-07\n",
      "Step 15100 of 89240, Loss:4.052758450508118, lr=2.473803661709374e-07\n",
      "Step 15200 of 89240, Loss:4.029460649490357, lr=2.3059915576050312e-07\n",
      "Step 15300 of 89240, Loss:4.044797749519348, lr=2.1495630983387216e-07\n",
      "Step 15400 of 89240, Loss:4.006130695343018, lr=2.0037460668496444e-07\n",
      "Step 15500 of 89240, Loss:4.009318418502808, lr=1.8678206299309795e-07\n",
      "Step 15600 of 89240, Loss:4.02207763671875, lr=1.741115784736583e-07\n",
      "Step 15700 of 89240, Loss:4.0052322673797605, lr=1.6230060463412424e-07\n",
      "Step 15800 of 89240, Loss:4.0231241035461425, lr=1.5129083600024658e-07\n",
      "Step 15900 of 89240, Loss:4.008826537132263, lr=1.4102792228810366e-07\n",
      "Step 16000 of 89240, Loss:4.001178331375122, lr=1.3146120010115474e-07\n",
      "Step 16100 of 89240, Loss:4.030073652267456, lr=1.225434428278014e-07\n",
      "Step 16200 of 89240, Loss:4.007132148742675, lr=1.1423062750481257e-07\n",
      "Step 16300 of 89240, Loss:3.989136748313904, lr=1.0648171749572306e-07\n",
      "Step 16400 of 89240, Loss:4.034935212135315, lr=9.92584599113866e-08\n",
      "Step 16500 of 89240, Loss:4.019076199531555, lr=9.252519677263908e-08\n",
      "Step 16600 of 89240, Loss:4.033656811714172, lr=8.624868898286728e-08\n",
      "Step 16700 of 89240, Loss:4.00285388469696, lr=8.039795224151449e-08\n",
      "Step 16800 of 89240, Loss:3.9802635097503662, lr=7.494410408850206e-08\n",
      "Step 16900 of 89240, Loss:4.0333508157730105, lr=6.986022132449316e-08\n",
      "Step 17000 of 89240, Loss:4.016402401924133, lr=6.512120710314725e-08\n",
      "Step 17100 of 89240, Loss:4.0179233598709105, lr=6.07036670392593e-08\n",
      "Step 17200 of 89240, Loss:4.013712487220764, lr=5.658579372118499e-08\n",
      "Step 17300 of 89240, Loss:4.019535870552063, lr=5.2747259057441753e-08\n",
      "Step 17400 of 89240, Loss:3.998189377784729, lr=4.9169113926049324e-08\n",
      "Step 17500 of 89240, Loss:4.004734296798706, lr=4.5833694631223385e-08\n",
      "Step 17600 of 89240, Loss:4.018467617034912, lr=4.272453570564123e-08\n",
      "Step 17700 of 89240, Loss:3.9959703493118286, lr=3.9826288627823184e-08\n",
      "Step 17800 of 89240, Loss:4.028057379722595, lr=3.712464605337417e-08\n",
      "Step 17900 of 89240, Loss:4.01325376033783, lr=3.4606271186049056e-08\n",
      "Step 18000 of 89240, Loss:4.003405766487122, lr=3.225873193997827e-08\n",
      "Step 18100 of 89240, Loss:4.038759970664978, lr=3.007043956804236e-08\n",
      "Step 18200 of 89240, Loss:4.041605796813965, lr=2.8030591453431345e-08\n",
      "Step 18300 of 89240, Loss:4.010702533721924, lr=2.612911778197627e-08\n",
      "Step 18400 of 89240, Loss:4.004623928070068, lr=2.435663183199841e-08\n",
      "Step 18500 of 89240, Loss:4.016162753105164, lr=2.2704383636279367e-08\n",
      "Step 18600 of 89240, Loss:4.011593117713928, lr=2.1164216787401986e-08\n",
      "Step 18700 of 89240, Loss:3.996980061531067, lr=1.9728528173229493e-08\n",
      "Step 18800 of 89240, Loss:4.024163751602173, lr=1.8390230443754958e-08\n",
      "Step 18900 of 89240, Loss:4.0251804637908934, lr=1.714271702403684e-08\n",
      "Step 19000 of 89240, Loss:4.021514472961425, lr=1.5979829500505098e-08\n",
      "Step 19100 of 89240, Loss:4.004969682693481, lr=1.4895827219638781e-08\n",
      "Step 19200 of 89240, Loss:4.026236305236816, lr=1.3885358948937354e-08\n",
      "Step 19300 of 89240, Loss:4.021832103729248, lr=1.294343646028878e-08\n",
      "Step 19400 of 89240, Loss:4.012659978866577, lr=1.2065409905327236e-08\n",
      "Step 19500 of 89240, Loss:3.9879958248138427, lr=1.1246944861219699e-08\n",
      "Step 19600 of 89240, Loss:4.029240183830261, lr=1.0484000933566748e-08\n",
      "Step 19700 of 89240, Loss:4.032010378837586, lr=9.772811810789701e-09\n",
      "Step 19800 of 89240, Loss:4.019821453094482, lr=9.109866671541596e-09\n",
      "Step 19900 of 89240, Loss:4.020949845314026, lr=8.491892853358678e-09\n",
      "Step 20000 of 89240, Loss:4.006777248382568, lr=7.915839696995387e-09\n",
      "Step 20100 of 89240, Loss:4.0213669538497925, lr=7.378863486689519e-09\n",
      "Step 20200 of 89240, Loss:4.042918672561646, lr=6.878313412014454e-09\n",
      "Step 20300 of 89240, Loss:4.036323504447937, lr=6.4117184820184e-09\n",
      "Step 20400 of 89240, Loss:3.966598701477051, lr=5.976775327051634e-09\n",
      "Step 20500 of 89240, Loss:4.013357520103455, lr=5.57133682806485e-09\n",
      "Step 20600 of 89240, Loss:4.002802157402039, lr=5.193401517246551e-09\n",
      "Step 20700 of 89240, Loss:3.981434049606323, lr=4.841103697675201e-09\n",
      "Step 20800 of 89240, Loss:4.018818717002869, lr=4.512704233211301e-09\n",
      "Step 20900 of 89240, Loss:4.008072462081909, lr=4.206581963163205e-09\n",
      "Step 21000 of 89240, Loss:4.008081707954407, lr=3.921225699344751e-09\n",
      "Step 21100 of 89240, Loss:4.014789776802063, lr=3.6552267660177733e-09\n",
      "Step 21200 of 89240, Loss:3.985008854866028, lr=3.4072720458925273e-09\n",
      "Step 21300 of 89240, Loss:4.023726119995117, lr=3.1761374978572806e-09\n",
      "Step 21400 of 89240, Loss:3.9927971601486205, lr=2.9606821144369813e-09\n",
      "Step 21500 of 89240, Loss:3.993034071922302, lr=2.7598422891517146e-09\n",
      "Step 21600 of 89240, Loss:4.024707288742065, lr=2.57262656596911e-09\n",
      "Step 21700 of 89240, Loss:4.036966595649719, lr=2.3981107449311165e-09\n",
      "Step 21800 of 89240, Loss:4.012531642913818, lr=2.235433319793809e-09\n",
      "Step 21900 of 89240, Loss:4.011957817077636, lr=2.083791225157915e-09\n",
      "Step 22000 of 89240, Loss:4.014547266960144, lr=1.9424358720955427e-09\n",
      "Step 22100 of 89240, Loss:4.033652863502502, lr=1.8106694527027967e-09\n",
      "Step 22200 of 89240, Loss:4.023363614082337, lr=1.687841495335494e-09\n",
      "Step 22300 of 89240, Loss:4.022372994422913, lr=1.5733456535227474e-09\n",
      "Step 22400 of 89240, Loss:4.016867737770081, lr=1.4666167127067104e-09\n",
      "Step 22500 of 89240, Loss:4.039390306472779, lr=1.3671278000321113e-09\n",
      "Step 22600 of 89240, Loss:4.005943608283997, lr=1.2743877834115507e-09\n",
      "Step 22700 of 89240, Loss:4.0009948253631595, lr=1.1879388470269274e-09\n",
      "Step 22800 of 89240, Loss:4.023334379196167, lr=1.1073542312983182e-09\n",
      "Step 22900 of 89240, Loss:3.998215389251709, lr=1.0322361261635668e-09\n",
      "Step 23000 of 89240, Loss:4.027872262001037, lr=9.622137072686375e-10\n",
      "Step 23100 of 89240, Loss:4.013543543815612, lr=8.96941305374295e-10\n",
      "Step 23200 of 89240, Loss:3.9887083053588865, lr=8.360966999422898e-10\n",
      "Step 23300 of 89240, Loss:4.018484220504761, lr=7.793795284772504e-10\n",
      "Step 23400 of 89240, Loss:4.020306873321533, lr=7.26509803771918e-10\n",
      "Step 23500 of 89240, Loss:4.025605864524842, lr=6.7722653173603e-10\n",
      "Step 23600 of 89240, Loss:4.032983322143554, lr=6.312864229856933e-10\n",
      "Step 23700 of 89240, Loss:4.031827669143677, lr=5.884626918329421e-10\n",
      "Step 23800 of 89240, Loss:4.018928217887878, lr=5.485439367466327e-10\n",
      "Step 23900 of 89240, Loss:4.02288242816925, lr=5.113330967580118e-10\n",
      "Step 24000 of 89240, Loss:4.017531442642212, lr=4.766464786592013e-10\n",
      "Step 24100 of 89240, Loss:3.991573214530945, lr=4.4431285019231796e-10\n",
      "Step 24200 of 89240, Loss:4.00506450176239, lr=4.141725947527049e-10\n",
      "Step 24300 of 89240, Loss:4.019512619972229, lr=3.8607692343342934e-10\n",
      "Step 24400 of 89240, Loss:4.031115202903748, lr=3.5988714052126116e-10\n",
      "Step 24500 of 89240, Loss:4.033834781646728, lr=3.354739588182165e-10\n",
      "Step 24600 of 89240, Loss:4.0078855943679805, lr=3.1271686140871625e-10\n",
      "Step 24700 of 89240, Loss:4.0050281095504765, lr=2.9150350672169096e-10\n",
      "Step 24800 of 89240, Loss:3.9940562009811402, lr=2.717291739506901e-10\n",
      "Step 24900 of 89240, Loss:4.010153594017029, lr=2.532962460942847e-10\n",
      "Step 25000 of 89240, Loss:4.020705409049988, lr=2.3611372806476494e-10\n",
      "Step 25100 of 89240, Loss:4.017619652748108, lr=2.2009679748625254e-10\n",
      "Step 25200 of 89240, Loss:4.027551026344299, lr=2.0516638596471979e-10\n",
      "Step 25300 of 89240, Loss:4.006868414878845, lr=1.912487887628331e-10\n",
      "Step 25400 of 89240, Loss:3.9955671548843386, lr=1.7827530095276108e-10\n",
      "Step 25500 of 89240, Loss:4.0331055498123165, lr=1.6618187825079708e-10\n",
      "Step 25600 of 89240, Loss:4.0301081085205075, lr=1.5490882085948897e-10\n",
      "Step 25700 of 89240, Loss:4.021387710571289, lr=1.4440047875654662e-10\n",
      "Step 25800 of 89240, Loss:4.033297185897827, lr=1.3460497697567107e-10\n",
      "Step 25900 of 89240, Loss:4.023051886558533, lr=1.2547395952313983e-10\n",
      "Step 26000 of 89240, Loss:4.031634373664856, lr=1.1696235066597945e-10\n",
      "Step 26100 of 89240, Loss:4.044302930831909, lr=1.090281324133128e-10\n",
      "Step 26200 of 89240, Loss:3.9865083408355715, lr=1.0163213709240587e-10\n",
      "Step 26300 of 89240, Loss:4.043977122306824, lr=9.473785399545518e-11\n",
      "Step 26400 of 89240, Loss:4.024163417816162, lr=8.831124914261818e-11\n",
      "Step 26500 of 89240, Loss:4.028783521652222, lr=8.232059727153746e-11\n",
      "Step 26600 of 89240, Loss:4.022490687370301, lr=7.673632522396625e-11\n",
      "Step 26700 of 89240, Loss:4.020664682388306, lr=7.153086595636582e-11\n",
      "Step 26800 of 89240, Loss:4.026856570243836, lr=6.66785224537901e-11\n",
      "Step 26900 of 89240, Loss:4.022337007522583, lr=6.2155340875262e-11\n",
      "Step 27000 of 89240, Loss:4.012614698410034, lr=5.793899230441661e-11\n",
      "Step 27100 of 89240, Loss:4.006683678627014, lr=5.400866252166778e-11\n",
      "Step 27200 of 89240, Loss:4.012234706878662, lr=5.034494925375233e-11\n",
      "Step 27300 of 89240, Loss:4.033825936317444, lr=4.692976639341946e-11\n",
      "Step 27400 of 89240, Loss:4.020315885543823, lr=4.3746254716440554e-11\n",
      "Step 27500 of 89240, Loss:4.006085624694824, lr=4.077869865518961e-11\n",
      "Step 27600 of 89240, Loss:4.003296837806702, lr=3.801244871794287e-11\n",
      "Step 27700 of 89240, Loss:4.040271172523498, lr=3.5433849170916303e-11\n",
      "Step 27800 of 89240, Loss:4.03060658454895, lr=3.303017062603995e-11\n",
      "Step 27900 of 89240, Loss:4.014913506507874, lr=3.078954720168493e-11\n",
      "Step 28000 of 89240, Loss:3.9960173892974855, lr=2.8700917946134188e-11\n",
      "Step 28100 of 89240, Loss:4.016678848266602, lr=2.675397223463062e-11\n",
      "Step 28200 of 89240, Loss:4.010945158004761, lr=2.4939098870452543e-11\n",
      "Step 28300 of 89240, Loss:4.045077719688416, lr=2.3247338638751278e-11\n",
      "Step 28400 of 89240, Loss:4.028382606506348, lr=2.1670340078930495e-11\n",
      "Step 28500 of 89240, Loss:4.027471733093262, lr=2.02003182572354e-11\n",
      "Step 28600 of 89240, Loss:4.019300298690796, lr=1.883001633603051e-11\n",
      "Step 28700 of 89240, Loss:4.023069686889649, lr=1.7552669750050857e-11\n",
      "Step 28800 of 89240, Loss:4.015527172088623, lr=1.636197281278085e-11\n",
      "Step 28900 of 89240, Loss:4.011101202964783, lr=1.5252047588111426e-11\n",
      "Step 29000 of 89240, Loss:3.999904165267944, lr=1.421741487360894e-11\n",
      "Step 29100 of 89240, Loss:3.9992574214935304, lr=1.325296715215311e-11\n",
      "Step 29200 of 89240, Loss:4.001946887969971, lr=1.235394337841846e-11\n",
      "Step 29300 of 89240, Loss:4.013353500366211, lr=1.1515905475731469e-11\n",
      "Step 29400 of 89240, Loss:3.99311155796051, lr=1.07347164272789e-11\n",
      "Step 29500 of 89240, Loss:4.03774664402008, lr=1.0006519853513474e-11\n",
      "Step 29600 of 89240, Loss:4.030382785797119, lr=9.327720974939715e-12\n",
      "Step 29700 of 89240, Loss:3.972830557823181, lr=8.69496886630178e-12\n",
      "Step 29800 of 89240, Loss:3.982778549194336, lr=8.10513991457017e-12\n",
      "Step 29900 of 89240, Loss:4.044411487579346, lr=7.55532239906683e-12\n",
      "Step 30000 of 89240, Loss:4.025241446495056, lr=7.042802117607634e-12\n",
      "Step 30100 of 89240, Loss:4.018031210899353, lr=6.565048987704995e-12\n",
      "Step 30200 of 89240, Loss:4.030772986412049, lr=6.119704556686731e-12\n",
      "Step 30300 of 89240, Loss:4.013006572723389, lr=5.704570359074251e-12\n",
      "Step 30400 of 89240, Loss:4.023300886154175, lr=5.317597063745698e-12\n",
      "Step 30500 of 89240, Loss:4.024654221534729, lr=4.956874357308425e-12\n",
      "Step 30600 of 89240, Loss:4.009371018409729, lr=4.620621513739583e-12\n",
      "Step 30700 of 89240, Loss:3.997357940673828, lr=4.3071786037413636e-12\n",
      "Step 30800 of 89240, Loss:4.032421398162842, lr=4.014998300415433e-12\n",
      "Step 30900 of 89240, Loss:4.0138487100601195, lr=3.742638240804837e-12\n",
      "Step 31000 of 89240, Loss:4.003745894432068, lr=3.4887539055957717e-12\n",
      "Step 31100 of 89240, Loss:4.036192650794983, lr=3.252091981829466e-12\n",
      "Step 31200 of 89240, Loss:4.043253383636475, lr=3.031484175858897e-12\n",
      "Step 31300 of 89240, Loss:4.027466697692871, lr=2.8258414460076593e-12\n",
      "Step 31400 of 89240, Loss:4.0398499250411986, lr=2.634148626460238e-12\n",
      "Step 31500 of 89240, Loss:4.030774378776551, lr=2.455459415844222e-12\n",
      "Step 31600 of 89240, Loss:4.022435116767883, lr=2.2888917057653577e-12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m train_loss_saver \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m----> 5\u001b[0m flowcode\u001b[39m.\u001b[39;49mtrain_flow(model, Data_flow, COND_INDS, N_EPOCHS, lr\u001b[39m=\u001b[39;49mINIT_LR, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, loss_saver\u001b[39m=\u001b[39;49mtrain_loss_saver)\n\u001b[1;32m      6\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m      7\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/work/flowcode.py:443\u001b[0m, in \u001b[0;36mtrain_flow\u001b[0;34m(flow_obj, data, cond_indx, epochs, optimizer_obj, lr, batch_size, loss_saver, print_fn, **print_fn_kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39m#zero_grad() on model (sometimes safer that optimizer.zero_grad())\u001b[39;00m\n\u001b[1;32m    442\u001b[0m flow_obj\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 443\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    444\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    446\u001b[0m \u001b[39mif\u001b[39;00m ct \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#...OR train here\n",
    "import time\n",
    "train_loss_saver = []\n",
    "start = time.perf_counter()\n",
    "flowcode.train_flow(model, Data_flow, COND_INDS, N_EPOCHS, lr=INIT_LR, batch_size=BATCH_SIZE, loss_saver=train_loss_saver)\n",
    "end = time.perf_counter()\n",
    "torch.save(model.state_dict(), f\"{filename}.pth\")\n",
    "np.save(f\"loss_{filename}.npy\",np.array(train_loss_saver+[end-start]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in training results:\n",
    "model.load_state_dict(torch.load(f\"{filename}.pth\"))\n",
    "loss_results = np.load(f\"loss_{filename}.npy\")\n",
    "loss_results, tot_time = loss_results[:-1], loss_results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a sample from the flow\n",
    "\n",
    "#Set a condition for the sample\n",
    "condition = mpc.diststack(Data_const)[:,COND_INDS]\n",
    "\n",
    "#Get sample\n",
    "flow_sample = mpc.galaxysplit(mpc.sample_to_Data(mpc.sample_Conditional(model, COND_INDS, condition)), N_stars_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get multiple galaxy plot\n",
    "visual.plot_conditional(flow_sample, M_stars_const, type=\"ofe\", show=\"page\", label=filename, scale=\"lin\", gridsize=100, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get comparison plot of single galaxy\n",
    "\n",
    "visual.get_result_plots(Data_const[50], flow_sample[50], label=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
