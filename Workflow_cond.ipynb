{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowcode\n",
    "import processing\n",
    "import res_flow_vis as visual\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filename associated with this specific run\n",
    "filename = \"testrun2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate a processor to handle data\n",
    "mpc = processing.Processor_cond(N_min=500, percentile2=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load raw data\n",
    "Data, N_stars, M_stars, M_dm = mpc.get_data(\"all_sims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut out 0 of 95 galaxies, 4417968 of 34878379 stars (~13%).\n"
     ]
    }
   ],
   "source": [
    "#Clean data\n",
    "Data_const, N_stars_const, M_stars_const, M_dm_const = mpc.constraindata(Data, M_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose device\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters of the flow\n",
    "LAYER_TYPE = flowcode.NSF_CL2\n",
    "N_LAYERS = 8\n",
    "COND_INDS = np.array([10])\n",
    "DIM_COND = COND_INDS.shape[0]\n",
    "DIM_NOTCOND = Data_const[0].shape[1] - DIM_COND\n",
    "SPLIT = 0.5\n",
    "K = 8\n",
    "B = 3\n",
    "BASE_NETWORK = flowcode.MLP\n",
    "BASE_NETWORK_N_LAYERS = 4\n",
    "BASE_NETWORK_N_HIDDEN = 16\n",
    "BASE_NETWORK_LEAKY_RELU_SLOPE = 0.2\n",
    "\n",
    "SPLIT = {\"split\":SPLIT} if LAYER_TYPE == flowcode.NSF_CL else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "model = flowcode.NSFlow(N_LAYERS, DIM_NOTCOND, DIM_COND, LAYER_TYPE, **SPLIT, K=K, B=B, network=BASE_NETWORK, network_args=(BASE_NETWORK_N_HIDDEN,BASE_NETWORK_N_LAYERS,BASE_NETWORK_LEAKY_RELU_SLOPE))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training hyperparameters\n",
    "N_EPOCHS = 3\n",
    "INIT_LR = 0.0002\n",
    "BATCH_SIZE = 1024\n",
    "LOG_LEARN = np.array([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare data for flow\n",
    "Data_flow = mpc.Data_to_flow(mpc.diststack(Data_const), LOG_LEARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save relevant data to the drive for external python file (device needs to be GPU) to do the training in background...\n",
    "torch.save(Data_flow, \"data_cond_trainer.pth\")\n",
    "torch.save(model, \"model_cond_trainer.pth\")\n",
    "np.save(\"params_cond_trainer.npy\", np.append(COND_INDS,np.array([N_EPOCHS,INIT_LR,BATCH_SIZE])))\n",
    "np.save(\"filename_cond_trainer.npy\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 89240, Loss:15.25014877319336, lr=0.0002\n",
      "Step 100 of 89240, Loss:14.914491596221923, lr=0.00019800897604194966\n",
      "Step 200 of 89240, Loss:14.58655683517456, lr=0.00019603777296590692\n",
      "Step 300 of 89240, Loss:14.163960170745849, lr=0.00019408619345261712\n",
      "Step 400 of 89240, Loss:13.887592811584472, lr=0.00019215404214716232\n",
      "Step 500 of 89240, Loss:13.570443820953368, lr=0.00019024112563940622\n",
      "Step 600 of 89240, Loss:13.150003070831298, lr=0.0001883472524446336\n",
      "Step 700 of 89240, Loss:12.606537399291993, lr=0.00018647223298438246\n",
      "Step 800 of 89240, Loss:11.904420547485351, lr=0.00018461587956746718\n",
      "Step 900 of 89240, Loss:11.08151174545288, lr=0.00018277800637119036\n",
      "Step 1000 of 89240, Loss:10.494283924102783, lr=0.00018095842942274178\n",
      "Step 1100 of 89240, Loss:9.952074069976806, lr=0.00017915696658078257\n",
      "Step 1200 of 89240, Loss:9.697899208068847, lr=0.0001773734375172127\n",
      "Step 1300 of 89240, Loss:9.435991744995118, lr=0.00017560766369912012\n",
      "Step 1400 of 89240, Loss:9.246514091491699, lr=0.00017385946837090917\n",
      "Step 1500 of 89240, Loss:9.047532844543458, lr=0.00017212867653660724\n",
      "Step 1600 of 89240, Loss:8.87954128265381, lr=0.00017041511494234786\n",
      "Step 1700 of 89240, Loss:8.729743041992187, lr=0.00016871861205902726\n",
      "Step 1800 of 89240, Loss:8.543639488220215, lr=0.00016703899806513463\n",
      "Step 1900 of 89240, Loss:8.408545036315918, lr=0.00016537610482975255\n",
      "Step 2000 of 89240, Loss:8.287163696289063, lr=0.00016372976589572715\n",
      "Step 2100 of 89240, Loss:8.101295766830445, lr=0.0001620998164630053\n",
      "Step 2200 of 89240, Loss:8.027005710601806, lr=0.00016048609337213828\n",
      "Step 2300 of 89240, Loss:7.85077561378479, lr=0.0001588884350879491\n",
      "Step 2400 of 89240, Loss:7.755375995635986, lr=0.00015730668168336293\n",
      "Step 2500 of 89240, Loss:7.625212812423706, lr=0.00015574067482339803\n",
      "Step 2600 of 89240, Loss:7.525976600646973, lr=0.0001541902577493165\n",
      "Step 2700 of 89240, Loss:7.4169667530059815, lr=0.00015265527526293226\n",
      "Step 2800 of 89240, Loss:7.222311820983887, lr=0.00015113557371107596\n",
      "Step 2900 of 89240, Loss:7.174247732162476, lr=0.00014963100097021377\n",
      "Step 3000 of 89240, Loss:7.089489164352417, lr=0.00014814140643122002\n",
      "Step 3100 of 89240, Loss:7.012156181335449, lr=0.00014666664098430084\n",
      "Step 3200 of 89240, Loss:6.947243051528931, lr=0.00014520655700406827\n",
      "Step 3300 of 89240, Loss:6.875815963745117, lr=0.00014376100833476277\n",
      "Step 3400 of 89240, Loss:6.807589149475097, lr=0.00014232985027562276\n",
      "Step 3500 of 89240, Loss:6.759218034744262, lr=0.00014091293956640033\n",
      "Step 3600 of 89240, Loss:6.707500667572021, lr=0.00013951013437302036\n",
      "Step 3700 of 89240, Loss:6.644373512268066, lr=0.00013812129427338282\n",
      "Step 3800 of 89240, Loss:6.653823709487915, lr=0.00013674628024330668\n",
      "Step 3900 of 89240, Loss:6.564194812774658, lr=0.00013538495464261317\n",
      "Step 4000 of 89240, Loss:6.521663837432861, lr=0.00013403718120134813\n",
      "Step 4100 of 89240, Loss:6.4874600982666015, lr=0.00013270282500614107\n",
      "Step 4200 of 89240, Loss:6.487615547180176, lr=0.00013138175248670012\n",
      "Step 4300 of 89240, Loss:6.417926692962647, lr=0.0001300738314024418\n",
      "Step 4400 of 89240, Loss:6.411549034118653, lr=0.00012877893082925354\n",
      "Step 4500 of 89240, Loss:6.336791973114014, lr=0.00012749692114638776\n",
      "Step 4600 of 89240, Loss:6.317987842559814, lr=0.00012622767402348717\n",
      "Step 4700 of 89240, Loss:6.335411329269409, lr=0.00012497106240773852\n",
      "Step 4800 of 89240, Loss:6.301149635314942, lr=0.00012372696051115442\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected value argument (Tensor of shape (1024, 10)) to be within the support (Real()) of the distribution Normal(loc: torch.Size([10]), scale: torch.Size([10])), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n       grad_fn=<CatBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m train_loss_saver \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m----> 5\u001b[0m flowcode\u001b[39m.\u001b[39;49mtrain_flow(model, Data_flow, COND_INDS, N_EPOCHS, lr\u001b[39m=\u001b[39;49mINIT_LR, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, loss_saver\u001b[39m=\u001b[39;49mtrain_loss_saver)\n\u001b[1;32m      6\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m      7\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/work/flowcode.py:435\u001b[0m, in \u001b[0;36mtrain_flow\u001b[0;34m(flow_obj, data, cond_indx, epochs, optimizer_obj, lr, batch_size, loss_saver, print_fn, **print_fn_kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m x \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    434\u001b[0m \u001b[39m#Evaluate model\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m z, logdet, prior_z_logprob \u001b[39m=\u001b[39m flow_obj(x[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m,\u001b[39m~\u001b[39;49mmask_cond],x[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m,mask_cond])\n\u001b[1;32m    437\u001b[0m \u001b[39m#Get loss\u001b[39;00m\n\u001b[1;32m    438\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmean(logdet\u001b[39m+\u001b[39mprior_z_logprob)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/flowcode.py:385\u001b[0m, in \u001b[0;36mNSFlow.forward\u001b[0;34m(self, x, x_cond)\u001b[0m\n\u001b[1;32m    382\u001b[0m     logdet \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m logdet_temp\n\u001b[1;32m    384\u001b[0m \u001b[39m#Get p_z(f(x)) which is needed for loss function together with logdet\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m prior_z_logprob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprior\u001b[39m.\u001b[39;49mlog_prob(x)\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    387\u001b[0m \u001b[39mreturn\u001b[39;00m x, logdet, prior_z_logprob\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/distributions/normal.py:79\u001b[0m, in \u001b[0;36mNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[1;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_args:\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_sample(value)\n\u001b[1;32m     80\u001b[0m     \u001b[39m# compute the variance\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     var \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/distributions/distribution.py:294\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    292\u001b[0m valid \u001b[39m=\u001b[39m support\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m--> 294\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    295\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected value argument \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto be within the support (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(support)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof the distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected value argument (Tensor of shape (1024, 10)) to be within the support (Real()) of the distribution Normal(loc: torch.Size([10]), scale: torch.Size([10])), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n       grad_fn=<CatBackward0>)"
     ]
    }
   ],
   "source": [
    "#...OR train here\n",
    "import time\n",
    "train_loss_saver = []\n",
    "start = time.perf_counter()\n",
    "flowcode.train_flow(model, Data_flow, COND_INDS, N_EPOCHS, lr=INIT_LR, batch_size=BATCH_SIZE, loss_saver=train_loss_saver)\n",
    "end = time.perf_counter()\n",
    "torch.save(model.state_dict(), f\"{filename}.pth\")\n",
    "np.save(f\"loss_{filename}.npy\",np.array(train_loss_saver+[end-start]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in training results:\n",
    "model.load_state_dict(torch.load(f\"{filename}.pth\"))\n",
    "loss_results = np.load(f\"loss_{filename}.npy\")\n",
    "loss_results, tot_time = loss_results[:-1], loss_results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a sample from the flow\n",
    "\n",
    "#Set a condition for the sample\n",
    "condition = mpc.diststack(Data_const)[:,COND_INDS]\n",
    "\n",
    "#Get sample\n",
    "flow_sample = mpc.galaxysplit(mpc.sample_to_Data(mpc.sample_Conditional(model, COND_INDS, condition)), N_stars_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get multiple galaxy plot (data)\n",
    "vs_data = visual.plot_conditional(Data_const, M_dm_const+M_stars_const, type=\"N\", label=filename+\"_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get multiple galaxy plot (sample)\n",
    "visual.plot_conditional(Data_const, M_dm_const+M_stars_const, type=\"N\", label=filename, v_pre=vs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get comparison plot of single galaxy\n",
    "\n",
    "visual.get_result_plots(Data_const[50], flow_sample[50], label=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual.loss_plot(loss_results, tot_time, savefig=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
