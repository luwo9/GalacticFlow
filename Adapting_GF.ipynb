{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapting GalacticFlow code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebbok is meant to provide some instructions/tips on how to adapt the code for your own purposes.\n",
    "\n",
    "For many cases it may suffice to use the existing code but in some cases you may want to make changes.\n",
    "\n",
    "Although this code was not designed to be adaptable originally, and e.g. dpendencies are not perfectly seperated, some effort has been made to make it easier to adapt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basic structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At it's core of of course this model has a **Normalizing Flow**, most likely implemented as a pytorch `nn.Module` subclass. However there is alot of othersteps involved for GalacticFlow to properly learn the Galaxies.\n",
    "\n",
    "As this model works with physical Data there is alot to process and prepare:\n",
    "\n",
    "- Data loading/cleaning\n",
    "- Chosing the right subset of data (Training/Validation, Milky Ways/All Galaxies, Star properties)\n",
    "- **Normalizing the data**:\n",
    "    - Denormalizing the data after sampling\n",
    "    - Normalizing any conditions (=galactic parameters) before sampling\n",
    "    - Respecting the Jacobian of the Normalization when evaluating the pdf\n",
    "\n",
    "For this reason a **processor** class is used for all the models, to automate these steps.\n",
    "\n",
    "Those two are then combined in the GalacticFlow API, that we recommend using (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GalacticFlow API\n",
    "\n",
    "For a detailed documentation of the API see `API_Workflow.ipynb`.\n",
    "\n",
    "The API combines the **processor** and the **flow** into a single object, that can easily be loaded and saved, trained and evaluated.\n",
    "\n",
    "Therefore all parameters (including arguments to processor and flow) must be given from initialization and are saved with the model. (See \"definition dict\".)\n",
    "Of course the parameters stored there depend on the processor and flow used and if you change them you may need to change the definition dict and update the API dedicated loading/saving methods of processor and flow as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now describe how do adapt the normalizing flow and the processor, especially in context of the GalacticFlow API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Normalizing Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the oldest building block it is the least well-thought out for adaptability.\n",
    "\n",
    "If you re not interested in the API you probably want to either write your own flow from scratch and use it accordingly.\n",
    "The only exception is if you only want to use a different base network or a different Coupling layer. You can pass them simply as arguments.\n",
    "\n",
    "For using a custom flow for the API, most likely you also will want to write it from scratch, or at least change most methods.\n",
    "The methods needed for the API are:\n",
    "- forward(x, x_cond): The forward pass of the flow (normalizing direction), returning a tuple of:\n",
    "    1. The transformed data\n",
    "    2. The log-determinant of the Jacobian\n",
    "    3. The prior log-probability\n",
    "- backward(x, x_cond): The backward pass of the flow (sampling direction), returning a tuple of:\n",
    "    1. The transformed data\n",
    "    2. The log-determinant of the Jacobian\n",
    "- sample_Flow(number, x_cond): Samples `n` samples from the flow with the given conditions and returns them.\n",
    "- to(device): Although implemented in the base class, you may want to overwrite this to ensure everything is on the right device. (E.g. if using `torch.distributions.Normal`)\n",
    "- classmethod load_API(definition): A method to load and build up the flow from a definition dictionary of the API. Returns an instance of the flow, properly setup and usable (e.g. state_dict is loaded if given).\n",
    "- save_API(): A method to create a dictionary that can be used to load the flow again. Note that saving in the API already saves the hyperparameters, that were used in load_API, so you don't need to save them again. But togehter with those parameters the flow should be recreateable. (May just be a wrapper arround the `state_dict` method.)\n",
    "\n",
    "After having defined a new class you need to also update the definition dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Example flow:\n",
    "class BetterFlow(nn.Module):\n",
    "    def __init__(self, n_dim, n_cond, n_layers) -> None:\n",
    "        super().__init__()\n",
    "        self.n_dim = n_dim\n",
    "        self.n_cond = n_cond\n",
    "        self.n_layers = n_layers\n",
    "        ...\n",
    "\n",
    "    def forward(self, x, x_cond):\n",
    "        dummy_result = x\n",
    "        dummy_logdet = torch.zeros(x.shape[0])\n",
    "        dummy_prior = torch.zeros(x.shape[0])\n",
    "        return dummy_result, dummy_logdet, dummy_prior\n",
    "    \n",
    "    def backward(self, x, x_cond):\n",
    "        pass\n",
    "\n",
    "    def sample_Flow(self, number, x_cond):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def load_API(cls, definition):\n",
    "        n_dim = definition[\"dim_notcond\"]\n",
    "        n_cond = definition[\"dim_cond\"]\n",
    "        n_layers = definition[\"n_layers\"]\n",
    "        flow =  cls(n_dim, n_cond, n_layers)\n",
    "\n",
    "        #If this flow was saved before by the API i.e. a state_dict should be present:\n",
    "        is_loaded = \"was_saved\" in definition and definition[\"was_saved\"]\n",
    "\n",
    "        if is_loaded:\n",
    "            flow.load_state_dict(definition[\"flow_dict\"])\n",
    "\n",
    "    def save_API(self):\n",
    "        return {\"flow_dict\": self.state_dict()}\n",
    "    \n",
    "\n",
    "#Then the original definition dict:\n",
    "example_definition = {\n",
    "    #Processor to use (str as registered in func_handle)\n",
    "    \"processor\": \"Processor_cond\",\n",
    "    #Processor init args\n",
    "    \"processor_args\": {},\n",
    "    #Processor get_data args here folder name with data\n",
    "    \"processor_data\": {\"folder\": \"all_sims\"},\n",
    "    #Processor cleaning_function args\n",
    "    \"processor_clean\": {\"N_min\":500},\n",
    "    #Flow to use (str as registered in func_handle)\n",
    "    \"flow\": \"NSFlow\",\n",
    "    #Flow hyperparameters\n",
    "    \"flow_hyper\": {\"n_layers\":14, \"dim_notcond\": 10, \"dim_cond\": 4, \"CL\":\"NSF_CL2\", \"K\": 10, \"B\":3, \"network\":\"MLP\", \"network_args\":torch.tensor([128,4,0.2])},\n",
    "    #Parameters for choosing the subset of the data to use:\n",
    "    #cond_fn: The function that computes/determines the condition for each galaxy. (See Processor_cond.choose_subset() for details.)\n",
    "    #use_fn_constructor: The function that constructs the subset of the data to use. (See Processor_cond.choose_subset() for details.)\n",
    "    #Will be called with leavout_key and leavout_vals as kwargs. I.e. will leavout galaxies that have galaxy[\"galaxy\"][leavout_key] in leavout_vals.\n",
    "    #The remaining galaxies are used for training.\n",
    "    #use_fn_constructor is also once called with leavout_vals=[] to construct the full dataset for comparing (i.e. include validation set)\n",
    "    \"subset_params\": {\"cond_fn\": \"cond_M_stars_2age_avZ\", \"use_fn_constructor\": \"construct_all_galaxies_leavout\", \"leavout_key\": \"id\", \"leavout_vals\": [66, 20, 88, 48, 5]},\n",
    "    #Parameters to processor.Data_to_flow\n",
    "    #transformation_components[i] will be transformed with transformation_functions[i] and the corresponding inverse transformation is given by inverse_transformations[i]\n",
    "    #transformation_logdets[i] is the logdet of the transformation_functions[i], needed in case of pdf evaluation.\n",
    "    \"data_prep_args\": {\"transformation_functions\":(\"np.log10\",), \"transformation_components\":([\"M_stars\"],), \"inverse_transformations\":(\"10**x\",), \"transformation_logdets\":(\"logdet_log10\",)}\n",
    "}\n",
    "\n",
    "#Then the definition dict for the adapted flow:\n",
    "adapted_definition = copy.deepcopy(example_definition)\n",
    "adapted_definition[\"flow\"] = \"BetterFlow\"\n",
    "#For this to work, the BetterFlow class needs to be registered in func_handle:\n",
    "#Key: \"BetterFlow\", Value: BetterFlow\n",
    "\n",
    "#NOTE: the keys \"dim_notcond\" and \"dim_cond\" are required for the API to work properly regardless if your flow uses them in this format or not.\n",
    "flow_hyper_params = {\"n_layers\": 14, \"dim_notcond\": 10, \"dim_cond\": 4}\n",
    "adapted_definition[\"flow_hyper\"] = flow_hyper_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processor supplies alot of of functionallity that should be rather universal and useful to reuse even if not using the API.\n",
    "In this case you may proceed as follows, to get a working processor:\n",
    "\n",
    "- Subclass the `Processor_cond` class\n",
    "\n",
    "The nexts steps depend on what you are trying to do:\n",
    "Most likely you just want to change the `get_data` method to load your data accordingly and probably the `constraindata` method that cleans the data.\n",
    "(The latter will possibly be re-written to be more flexible, with the cleaning supplied as function argument.)\n",
    "\n",
    "This should be very easy, but there are some things necessary for the processor to work properly:\n",
    "\n",
    "- Use the right data structure (see `Base_Processor_Workflow.ipynb`, `API_Workflow.ipynb` and the `Processor_cond` class)\n",
    "- Properly register component and condtion names, as the processor keeps track of what components the data it is operating on has. \n",
    "\n",
    "See the `Processor_cond` code or the example below for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "#Suppose the data we want to load is given as a csv file where the colums are the stellar properties and the rows are the stars:\n",
    "# x, y, z, vx, vy, vz, mass, age, Teff, logg, feh\n",
    "# <star 1>\n",
    "# <star 2>\n",
    "# ...\n",
    "#And we have a numpy array of star numbers per galaxy, and we need to split the data into galaxies (As .npy file).\n",
    "#We have an array of same length with the DM halo mass of each galaxy (As .npy file).\n",
    "\n",
    "#Very likely you will need to change the constraindata method, as (right now) the component names and how to clean them are hard coded.\n",
    "\n",
    "#Here is an example of how to do this, with all necessary steps:\n",
    "class MyProcessor(processing.Processor_cond):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_data(self, stars, N_stars, halo_masses):\n",
    "\n",
    "        #First load the data e.g. with pandas:\n",
    "        data = pd.read_csv(stars)\n",
    "\n",
    "        N_stars = np.load(N_stars)\n",
    "        halo_masses = np.load(halo_masses)\n",
    "\n",
    "        #Now we need to have the right data structure:\n",
    "        # List of dicts one for each galaxy, keys \"stars\", \"galaxy\"\n",
    "        # Stars is a pd.DataFrame with the stellar properties as already loaded\n",
    "        # Galaxy is a dict with other information about the galaxy\n",
    "\n",
    "        Galaxies = []\n",
    "\n",
    "        for i, (n_star, halo_mass, n_star_already_loaded) in enumerate(zip(N_stars, halo_masses, np.cumsum(N_stars))):\n",
    "            #Get the stars for this galaxy:\n",
    "            stars = data.iloc[n_star_already_loaded - n_star : n_star_already_loaded]\n",
    "\n",
    "            #Get the galaxy properties, e.g.:\n",
    "            #Get the total mass of the stars:\n",
    "            total_mass = stars[\"mass\"].sum().values[0]\n",
    "            #Asssign a unique id to the galaxy:\n",
    "            unique_id = i\n",
    "            galaxy = {\"M_dm\": halo_mass, \"N_stars\": n_star, \"M_stars\": total_mass, \"id\": unique_id}\n",
    "            #You are free to choose whatever you want to store in the galaxy dict.\n",
    "            #Note however, that the standard cleaning function assumes that the galaxy dict contains the keys \"N_stars\" and \"M_stars\" to be present.\n",
    "            #And the standard use_fn for choosing the subset assumes to have an id assigned for selection train/test but you can just pass a different use_fn to the method.\n",
    "\n",
    "            #Maybe we decide that we dont actually ar interested in logg, so we drop it:\n",
    "            stars = stars.drop(columns=[\"logg\"])\n",
    "\n",
    "            #Now append the galaxy to the list of galaxies:\n",
    "            Galaxies.append({\"stars\": stars, \"galaxy\": galaxy})\n",
    "\n",
    "        #Now there just is one more important step:\n",
    "        # Register the component names:\n",
    "        used_component_names = data.drop(columns=[\"logg\"]).columns.to_list()\n",
    "        self.component_names[\"stars\"] = used_component_names\n",
    "\n",
    "        #If you were to also use e.g. gas you can just do the same as with stars but replacing \"stars\" with e.g. \"gas\" everywhere.\n",
    "\n",
    "        #Now we are done and can return the list of galaxies:\n",
    "        return Galaxies\n",
    "\n",
    "    def constraindata(self, Galaxies, m_max, M_dm_interval, make_copy=True):\n",
    "            #Say e.g. we are only interested in stars with mass < m_max\n",
    "            #And galaxies with DM halo mass in M_dm_interval\n",
    "\n",
    "            Galaxies_cleaned = []\n",
    "\n",
    "            for galaxy in Galaxies:\n",
    "                \n",
    "                #Clean out unwanted galaxies:\n",
    "                M_dm_max, M_dm_min = M_dm_interval\n",
    "                galaxy_is_valid = galaxy[\"galaxy\"][\"M_dm\"] < M_dm_max and galaxy[\"galaxy\"][\"M_dm\"] > M_dm_min\n",
    "\n",
    "                if galaxy_is_valid:\n",
    "                    galaxy_cleaned = copy.deepcopy(galaxy) if make_copy else galaxy\n",
    "                    \n",
    "                    #Clean the stars:\n",
    "                    stars_valid = galaxy_cleaned[\"stars\"][\"mass\"] < m_max\n",
    "                    galaxy_cleaned[\"stars\"] = galaxy_cleaned[\"stars\"][stars_valid]\n",
    "\n",
    "                    #You may need to update some parameters\n",
    "                    galaxy_cleaned[\"galaxy\"][\"N_stars\"] = stars_valid.sum()\n",
    "                    galaxy_cleaned[\"galaxy\"][\"M_stars\"] = galaxy_cleaned[\"stars\"][\"mass\"].sum().values[0]\n",
    "\n",
    "                    #You may also want to rotate the galaxies, such that their preferred axes are aligned\n",
    "                    galaxy_cleaned[\"stars\"] = processing.rotate_galaxy_xy(galaxy_cleaned[\"stars\"], quant=0.9)\n",
    "\n",
    "                    Galaxies_cleaned.append(galaxy_cleaned)\n",
    "\n",
    "                #(Note: You may need to swap Galaxy cleaning with star cleaning if e.g. you want to clean out galaxies with too few stars.)\n",
    "\n",
    "            return Galaxies_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the API, you will need to update the definition dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "#Then the original definition dict:\n",
    "example_definition = {\n",
    "    #Processor to use (str as registered in func_handle)\n",
    "    \"processor\": \"Processor_cond\",\n",
    "    #Processor init args\n",
    "    \"processor_args\": {},\n",
    "    #Processor get_data args here folder name with data\n",
    "    \"processor_data\": {\"folder\": \"all_sims\"},\n",
    "    #Processor cleaning_function args\n",
    "    \"processor_clean\": {\"N_min\":500},\n",
    "    #Flow to use (str as registered in func_handle)\n",
    "    \"flow\": \"NSFlow\",\n",
    "    #Flow hyperparameters\n",
    "    \"flow_hyper\": {\"n_layers\":14, \"dim_notcond\": 10, \"dim_cond\": 4, \"CL\":\"NSF_CL2\", \"K\": 10, \"B\":3, \"network\":\"MLP\", \"network_args\":torch.tensor([128,4,0.2])},\n",
    "    #Parameters for choosing the subset of the data to use:\n",
    "    #cond_fn: The function that computes/determines the condition for each galaxy. (See Processor_cond.choose_subset() for details.)\n",
    "    #use_fn_constructor: The function that constructs the subset of the data to use. (See Processor_cond.choose_subset() for details.)\n",
    "    #Will be called with leavout_key and leavout_vals as kwargs. I.e. will leavout galaxies that have galaxy[\"galaxy\"][leavout_key] in leavout_vals.\n",
    "    #The remaining galaxies are used for training.\n",
    "    #use_fn_constructor is also once called with leavout_vals=[] to construct the full dataset for comparing (i.e. include validation set)\n",
    "    \"subset_params\": {\"cond_fn\": \"cond_M_stars_2age_avZ\", \"use_fn_constructor\": \"construct_all_galaxies_leavout\", \"leavout_key\": \"id\", \"leavout_vals\": [66, 20, 88, 48, 5]},\n",
    "    #Parameters to processor.Data_to_flow\n",
    "    #transformation_components[i] will be transformed with transformation_functions[i] and the corresponding inverse transformation is given by inverse_transformations[i]\n",
    "    #transformation_logdets[i] is the logdet of the transformation_functions[i], needed in case of pdf evaluation.\n",
    "    \"data_prep_args\": {\"transformation_functions\":(\"np.log10\",), \"transformation_components\":([\"M_stars\"],), \"inverse_transformations\":(\"10**x\",), \"transformation_logdets\":(\"logdet_log10\",)}\n",
    "}\n",
    "\n",
    "#becomes: (e.g.)\n",
    "new_definition = copy.deepcopy(example_definition)\n",
    "new_definition[\"processor\"] = \"MyProcessor\"\n",
    "new_definition[\"processor_args\"] = {}\n",
    "new_definition[\"processor_data\"] = {\"stars\": \"all_stars.csv\", \"N_stars\": \"N_stars.npy\", \"halo_masses\": \"halo_masses.npy\"}\n",
    "new_definition[\"processor_clean\"] = {\"m_max\": 1e4, \"M_dm_interval\": (1e11, 1e12)}\n",
    "\n",
    "#However for this to work you need to add the new processor to the func_handle dict in API.py:\n",
    "#Key = \"MyProcessor\", Value = MyProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you change other things i.e. introduce parameters that need to be saved you may need to modify the `save_API` and `load_API` methods as well.\n",
    "\n",
    "See above for the analogous example of the flow.\n",
    "\n",
    "**NOTE:** The Key \"subset params\" with key \"leavout_key\" and \"leavout_vals\" is needed regardless of the processor used. Also the `.prepare` method of the API requires the full \"subset_params\" dict like above to be given and also \"processor_data\" \"processor_clean\" which is passed to the `get_data` and the `constraindata` method respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to change anything more fundamental beyond that you may need to change the API as well, e.g. the `.prepare` method that is responsible for how the data is obtained and handeled.\n",
    "\n",
    "You can again subclass from the `GalacticFlow` class and overwrite the corresponding methods."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
