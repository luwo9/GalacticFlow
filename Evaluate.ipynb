{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowcode\n",
    "import processing\n",
    "import res_flow_vis as visual\n",
    "import device_use\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate processing\n",
    "mp = processing.Processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rewrite the code to fit the new Processor Code in processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data raw data\n",
    "Data = mp.get_data(\"Old/g8.26e11.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean data\n",
    "Data_const = mp.constrain_data(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Right device\n",
    "device = device_use.device_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "model = flowcode.NSFlow(8, 10, 0, flowcode.NSF_CL2, K=8, B=3, network = flowcode.MLP, network_args=(16,4,0.2))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_flow = mp.Data_to_flow(Data_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save relevant data to the drive for external python file (device needs to be GPU) to do the training in background...\n",
    "torch.save(Data_flow, \"NC_trainer/data_NC_trainer.pth\")\n",
    "torch.save(model, \"NC_trainer/model_NC_trainer.pth\")\n",
    "np.save(\"NC_trainer/params_NC_trainer.npy\", np.array([150,0.0003,1024,0.999]))\n",
    "np.save(\"NC_trainer/filename_NC_trainer.npy\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 71402, Loss:16.726869583129883, lr=0.016\n",
      "Step 100 of 71402, Loss:7.617042808532715, lr=0.01568286469363119\n",
      "Step 200 of 71402, Loss:5.122779378890991, lr=0.01537201531242148\n",
      "Step 300 of 71402, Loss:4.765067081451416, lr=0.015067327263320803\n",
      "Step 400 of 71402, Loss:4.304386401176453, lr=0.01476867842283253\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m my_loss_saver \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m----> 5\u001b[0m flowcode\u001b[39m.\u001b[39;49mtrain_flow(model, Data_flow, np\u001b[39m.\u001b[39;49marray([]), \u001b[39m100\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.016\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m, loss_saver\u001b[39m=\u001b[39;49mmy_loss_saver, gamma\u001b[39m=\u001b[39;49m\u001b[39m0.998\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m      7\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msaves/\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/work/flowcode.py:449\u001b[0m, in \u001b[0;36mtrain_flow\u001b[0;34m(flow_obj, data, cond_indx, epochs, optimizer_obj, lr, batch_size, loss_saver, gamma, give_textfile_info, print_fn, **print_fn_kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    448\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> 449\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    451\u001b[0m \u001b[39mif\u001b[39;00m ct \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    452\u001b[0m     \u001b[39mif\u001b[39;00m give_textfile_info:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/radam.py:130\u001b[0m, in \u001b[0;36mRAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    127\u001b[0m             exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mexp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    128\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 130\u001b[0m     radam(params_with_grad,\n\u001b[1;32m    131\u001b[0m           grads,\n\u001b[1;32m    132\u001b[0m           exp_avgs,\n\u001b[1;32m    133\u001b[0m           exp_avg_sqs,\n\u001b[1;32m    134\u001b[0m           state_steps,\n\u001b[1;32m    135\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    136\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    137\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    138\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    139\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    140\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/radam.py:179\u001b[0m, in \u001b[0;36mradam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, state_steps, foreach, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_radam\n\u001b[0;32m--> 179\u001b[0m func(params,\n\u001b[1;32m    180\u001b[0m      grads,\n\u001b[1;32m    181\u001b[0m      exp_avgs,\n\u001b[1;32m    182\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    183\u001b[0m      state_steps,\n\u001b[1;32m    184\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    185\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    186\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    187\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    188\u001b[0m      eps\u001b[39m=\u001b[39;49meps)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/radam.py:233\u001b[0m, in \u001b[0;36m_single_tensor_radam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, state_steps, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m rho_t \u001b[39m>\u001b[39m \u001b[39m5.\u001b[39m:\n\u001b[1;32m    231\u001b[0m     \u001b[39m# Compute the variance rectification term and update parameters accordingly\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     rect \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt((rho_t \u001b[39m-\u001b[39m \u001b[39m4\u001b[39m) \u001b[39m*\u001b[39m (rho_t \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m rho_inf \u001b[39m/\u001b[39m ((rho_inf \u001b[39m-\u001b[39m \u001b[39m4\u001b[39m) \u001b[39m*\u001b[39m (rho_inf \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m rho_t))\n\u001b[0;32m--> 233\u001b[0m     adaptive_lr \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39;49msqrt(bias_correction2) \u001b[39m/\u001b[39;49m exp_avg_sq\u001b[39m.\u001b[39;49msqrt()\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[1;32m    235\u001b[0m     param\u001b[39m.\u001b[39madd_(bias_corrected_exp_avg \u001b[39m*\u001b[39m lr \u001b[39m*\u001b[39m adaptive_lr \u001b[39m*\u001b[39m rect, alpha\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:39\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     38\u001b[0m         \u001b[39mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:838\u001b[0m, in \u001b[0;36mTensor.__rdiv__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[39m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    837\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__rdiv__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m--> 838\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreciprocal() \u001b[39m*\u001b[39m other\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "#list to collect loss into\n",
    "my_loss_saver = []\n",
    "start = time.perf_counter()\n",
    "flowcode.train_flow(model, Data_flow, np.array([]), 100, lr=0.016, batch_size=1024, loss_saver=my_loss_saver, gamma=0.998)\n",
    "end = time.perf_counter()\n",
    "torch.save(model.state_dict(), f\"saves/{filename}.pth\")\n",
    "np.save(f\"saves/loss_{filename}.npy\",np.array(my_loss_saver+[end-start]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), f\"saves/{filename}.pth\")\n",
    "#np.save(f\"saves/loss_{filename}.npy\",np.array(my_loss_saver+[358*60]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in training results:\n",
    "model.load_state_dict(torch.load(f\"saves/{filename}.pth\"))\n",
    "loss_results = np.load(f\"saves/loss_{filename}.npy\")\n",
    "loss_results, tot_time = loss_results[:-1], loss_results[-1]/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample model\n",
    "N_samples = Data_const.shape[0]\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    res = (model.sample_Flow(N_samples, torch.tensor([]))).cpu()\n",
    "\n",
    "flow_sample = res\n",
    "flow_sample = mp.sample_to_Data(flow_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get result plots\n",
    "visual.get_result_plots(Data_const, flow_sample, label=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get loss curve\n",
    "visual.loss_plot(loss_results, tot_time=tot_time, savefig=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
